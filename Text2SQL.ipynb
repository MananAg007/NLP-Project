{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")\n",
    "!export CUDA_VISIBLE_DEVICES=0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tulip/Library/Python/3.10/lib/python/site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/tulip/Library/Python/3.10/lib/python/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers\n",
    "# helper for question to sql metadata data\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "import transformers\n",
    "\n",
    "pretrained_weights = {\n",
    "    (\"bert\", \"base\"): \"bert-base-uncased\",\n",
    "    (\"bert\", \"large\"): \"bert-large-uncased-whole-word-masking\",\n",
    "    (\"roberta\", \"base\"): \"roberta-base\",\n",
    "    (\"roberta\", \"large\"): \"roberta-large\",\n",
    "    (\"albert\", \"xlarge\"): \"albert-xlarge-v2\"\n",
    "}\n",
    "\n",
    "def create_base_model(config):\n",
    "    weights_name = pretrained_weights[(config[\"base_class\"], config[\"base_name\"])]\n",
    "    if config[\"base_class\"] == \"bert\":\n",
    "        return transformers.BertModel.from_pretrained(weights_name)\n",
    "    elif config[\"base_class\"] == \"roberta\":\n",
    "        return transformers.RobertaModel.from_pretrained(weights_name)\n",
    "    elif config[\"base_class\"] == \"albert\":\n",
    "        return transformers.AlbertModel.from_pretrained(weights_name)\n",
    "    else:\n",
    "        raise Exception(\"base_class {0} not supported\".format(config[\"base_class\"]))\n",
    "\n",
    "def create_tokenizer(config):\n",
    "    weights_name = pretrained_weights[(config[\"base_class\"], config[\"base_name\"])]\n",
    "    if config[\"base_class\"] == \"bert\":\n",
    "        return transformers.BertTokenizer.from_pretrained(weights_name)\n",
    "    elif config[\"base_class\"] == \"roberta\":\n",
    "        return transformers.RobertaTokenizer.from_pretrained(weights_name)\n",
    "    elif config[\"base_class\"] == \"albert\":\n",
    "        return transformers.AlbertTokenizer.from_pretrained(weights_name)\n",
    "    else:\n",
    "        raise Exception(\"base_class {0} not supported\".format(config[\"base_class\"]))\n",
    "\n",
    "def read_conf(conf_path):\n",
    "    config = {}\n",
    "    for line in open(conf_path, encoding=\"utf8\"):\n",
    "        if line.strip() == \"\" or line[0] == \"#\":\n",
    "             continue\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        config[fields[0]] = fields[1]\n",
    "    config[\"train_data_path\"] =  os.path.abspath(config[\"train_data_path\"])\n",
    "    config[\"dev_data_path\"] =  os.path.abspath(config[\"dev_data_path\"])\n",
    "\n",
    "    return config\n",
    "\n",
    "def read_jsonl(jsonl):\n",
    "    for line in open(jsonl, encoding=\"utf8\"):\n",
    "        sample = json.loads(line.rstrip())\n",
    "        yield sample\n",
    "\n",
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\n\" or c == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(c)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_punctuation(c):\n",
    "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "  cp = ord(c)\n",
    "  # We treat all non-letter/number ASCII as punctuation.\n",
    "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "  # Punctuation class but we treat them as punctuation anyways, for\n",
    "  # consistency.\n",
    "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "    return True\n",
    "  cat = unicodedata.category(c)\n",
    "  if cat.startswith(\"P\") or cat.startswith(\"S\"):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def basic_tokenize(doc):\n",
    "    doc_tokens = []\n",
    "    char_to_word = []\n",
    "    word_to_char_start = []\n",
    "    prev_is_whitespace = True\n",
    "    prev_is_punc = False\n",
    "    prev_is_num = False\n",
    "    for pos, c in enumerate(doc):\n",
    "        if is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "            prev_is_punc = False\n",
    "        else:\n",
    "            if prev_is_whitespace or is_punctuation(c) or prev_is_punc or (prev_is_num and not str(c).isnumeric()):\n",
    "                doc_tokens.append(c)\n",
    "                word_to_char_start.append(pos)\n",
    "            else:\n",
    "                doc_tokens[-1] += c\n",
    "            prev_is_whitespace = False\n",
    "            prev_is_punc = is_punctuation(c)\n",
    "            prev_is_num = str(c).isnumeric()\n",
    "        char_to_word.append(len(doc_tokens) - 1)\n",
    "\n",
    "    return doc_tokens, char_to_word, word_to_char_start\n",
    "\n",
    "class SQLExample(object):\n",
    "    def __init__(self,\n",
    "                 qid,\n",
    "                 question,\n",
    "                 table_id,\n",
    "                 column_meta,\n",
    "                 agg=None,\n",
    "                 select=None,\n",
    "                 conditions=None,\n",
    "                 tokens=None,\n",
    "                 char_to_word=None,\n",
    "                 word_to_char_start=None,\n",
    "                 value_start_end=None,\n",
    "                 valid=True):\n",
    "        self.qid = qid\n",
    "        self.question = question\n",
    "        self.table_id = table_id\n",
    "        self.column_meta = column_meta\n",
    "        self.agg = agg\n",
    "        self.select = select\n",
    "        self.conditions = conditions\n",
    "        self.valid = valid\n",
    "        if tokens is None:\n",
    "            self.tokens, self.char_to_word, self.word_to_char_start = basic_tokenize(question)\n",
    "            self.value_start_end = {}\n",
    "            if conditions is not None and len(conditions) > 0:\n",
    "                cur_start = None\n",
    "                for cond in conditions:\n",
    "                    value = cond[-1]\n",
    "                    value_tokens, _, _ = basic_tokenize(value)\n",
    "                    val_len = len(value_tokens)\n",
    "                    for i in range(len(self.tokens)):\n",
    "                        if \" \".join(self.tokens[i:i+val_len]).lower() != \" \".join(value_tokens).lower():\n",
    "                            continue\n",
    "                        s = self.word_to_char_start[i]\n",
    "                        e = len(question) if i + val_len >= len(self.word_to_char_start) else self.word_to_char_start[i + val_len]\n",
    "                        recovered_answer_text = question[s:e].strip()\n",
    "                        if value.lower() == recovered_answer_text.lower():\n",
    "                            cur_start = i\n",
    "                            break\n",
    "\n",
    "                    if cur_start is None:\n",
    "                        self.valid = False\n",
    "                        print([value, value_tokens, question, self.tokens])\n",
    "                        # for c in question:\n",
    "                        #     print((c, ord(c), unicodedata.category(c)))\n",
    "                        # raise Exception()\n",
    "                    else:\n",
    "                        self.value_start_end[value] = (cur_start, cur_start + val_len)\n",
    "        else:\n",
    "            self.tokens, self.char_to_word, self.word_to_char_start, self.value_start_end = tokens, char_to_word, word_to_char_start, value_start_end\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_json(s):\n",
    "        d = json.loads(s)\n",
    "        keys = [\"qid\", \"question\", \"table_id\", \"column_meta\", \"agg\", \"select\", \"conditions\", \"tokens\", \"char_to_word\", \"word_to_char_start\", \"value_start_end\", \"valid\"]\n",
    "\n",
    "        return SQLExample(*[d[k] for k in keys])\n",
    "\n",
    "    def dump_to_json(self):\n",
    "        d = {}\n",
    "        d[\"qid\"] = self.qid\n",
    "        d[\"question\"] = self.question\n",
    "        d[\"table_id\"] = self.table_id\n",
    "        d[\"column_meta\"] = self.column_meta\n",
    "        d[\"agg\"] = self.agg\n",
    "        d[\"select\"] = self.select\n",
    "        d[\"conditions\"] = self.conditions\n",
    "        d[\"tokens\"] = self.tokens\n",
    "        d[\"char_to_word\"] = self.char_to_word\n",
    "        d[\"word_to_char_start\"] = self.word_to_char_start\n",
    "        d[\"value_start_end\"] = self.value_start_end\n",
    "        d[\"valid\"] = self.valid\n",
    "\n",
    "        return json.dumps(d)\n",
    "\n",
    "    def output_SQ(self, return_str=True):\n",
    "        agg_ops = ['NA', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "        cond_ops = ['=', '>', '<', 'OP']\n",
    "\n",
    "        agg_text = agg_ops[self.agg]\n",
    "        select_text = self.column_meta[self.select][0]\n",
    "        cond_texts = []\n",
    "        for wc, op, value_text in self.conditions:\n",
    "            column_text = self.column_meta[wc][0]\n",
    "            op_text = cond_ops[op]\n",
    "            cond_texts.append(column_text + op_text + value_text)\n",
    "\n",
    "        if return_str:\n",
    "            sq = agg_text + \", \" + select_text + \", \" + \" AND \".join(cond_texts)\n",
    "        else:\n",
    "            sq = (agg_text, select_text, set(cond_texts))\n",
    "        return sq\n",
    "\n",
    "def get_schema(tables):\n",
    "    schema, headers, colTypes, naturalMap = {}, {}, {}, {}\n",
    "    for table in tables:\n",
    "        values = [set() for _ in range(len(table[\"header\"]))]\n",
    "        for row in table[\"rows\"]:\n",
    "            for i, value in enumerate(row):\n",
    "                values[i].add(str(value).lower())\n",
    "        columns = {column: values[i] for i, column in enumerate(table[\"header\"])}\n",
    "\n",
    "        trans = {\"text\": \"string\", \"real\": \"real\"}\n",
    "        colTypes[table[\"id\"]] = {col:trans[ty] for ty, col in zip(table[\"types\"], table[\"header\"])}\n",
    "        schema[table[\"id\"]] = columns\n",
    "        naturalMap[table[\"id\"]] = {col: col for col in columns}\n",
    "        headers[table[\"id\"]] = table[\"header\"]\n",
    "\n",
    "    return schema, headers, colTypes, naturalMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizer\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch.utils.data as torch_data\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "stats = defaultdict(int)\n",
    "\n",
    "class InputFeature(object):\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 table_id,\n",
    "                 tokens,\n",
    "                 word_to_char_start,\n",
    "                 word_to_subword,\n",
    "                 subword_to_word,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids):\n",
    "        self.question = question\n",
    "        self.table_id = table_id\n",
    "        self.tokens = tokens\n",
    "        self.word_to_char_start = word_to_char_start\n",
    "        self.word_to_subword = word_to_subword\n",
    "        self.subword_to_word = subword_to_word\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "\n",
    "        self.columns = None\n",
    "        self.agg = None\n",
    "        self.select = None\n",
    "        self.where_num = None\n",
    "        self.where = None\n",
    "        self.op = None\n",
    "        self.value_start = None\n",
    "        self.value_end = None\n",
    "\n",
    "    def output_SQ(self, agg = None, sel = None, conditions = None, return_str=True):\n",
    "        agg_ops = ['NA', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "        cond_ops = ['=', '>', '<', 'OP']\n",
    "\n",
    "        if agg is None and sel is None and conditions is None:\n",
    "            sel = np.argmax(self.select)\n",
    "            agg = self.agg[sel]\n",
    "            conditions = []\n",
    "            for i in range(len(self.where)):\n",
    "                if self.where[i] == 0:\n",
    "                    continue\n",
    "                conditions.append((i, self.op[i], self.value_start[i], self.value_end[i]))\n",
    "\n",
    "        agg_text = agg_ops[agg]\n",
    "        select_text = self.columns[sel]\n",
    "        cond_texts = []\n",
    "        for wc, op, vs, ve in conditions:\n",
    "            column_text = self.columns[wc]\n",
    "            op_text = cond_ops[op]\n",
    "            word_start, word_end = self.subword_to_word[wc][vs], self.subword_to_word[wc][ve]\n",
    "            char_start = self.word_to_char_start[word_start]\n",
    "            char_end = len(self.question) if word_end + 1 >= len(self.word_to_char_start) else self.word_to_char_start[word_end + 1]\n",
    "            value_span_text = self.question[char_start:char_end]\n",
    "            cond_texts.append(column_text + op_text + value_span_text.rstrip())\n",
    "\n",
    "        if return_str:\n",
    "            sq = agg_text + \", \" + select_text + \", \" + \" AND \".join(cond_texts)\n",
    "        else:\n",
    "            sq = (agg_text, select_text, set(cond_texts))\n",
    "\n",
    "        return sq\n",
    "\n",
    "class HydraFeaturizer(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = create_tokenizer(config)\n",
    "        self.colType2token = {\n",
    "            \"string\": \"[unused1]\",\n",
    "            \"real\": \"[unused2]\"}\n",
    "\n",
    "    def get_input_feature(self, example: SQLExample, config):\n",
    "        max_total_length = int(config[\"max_total_length\"])\n",
    "\n",
    "        input_feature = InputFeature(\n",
    "            example.question,\n",
    "            example.table_id,\n",
    "            [],\n",
    "            example.word_to_char_start,\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            []\n",
    "        )\n",
    "\n",
    "        for column, col_type, _ in example.column_meta:\n",
    "            # get query tokens\n",
    "            tokens = []\n",
    "            word_to_subword = []\n",
    "            subword_to_word = []\n",
    "            for i, query_token in enumerate(example.tokens):\n",
    "                if self.config[\"base_class\"] == \"roberta\":\n",
    "                    sub_tokens = self.tokenizer.tokenize(query_token, add_prefix_space=True)\n",
    "                else:\n",
    "                    sub_tokens = self.tokenizer.tokenize(query_token)\n",
    "                cur_pos = len(tokens)\n",
    "                if len(sub_tokens) > 0:\n",
    "                    word_to_subword += [(cur_pos, cur_pos + len(sub_tokens))]\n",
    "                    tokens.extend(sub_tokens)\n",
    "                    subword_to_word.extend([i] * len(sub_tokens))\n",
    "\n",
    "            if self.config[\"base_class\"] == \"roberta\":\n",
    "                tokenize_result = self.tokenizer.encode_plus(\n",
    "                    col_type + \" \" + column,\n",
    "                    tokens,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_total_length,\n",
    "                    truncation=True,\n",
    "                    add_prefix_space=True\n",
    "                )\n",
    "            else:\n",
    "                tokenize_result = self.tokenizer.encode_plus(\n",
    "                    col_type + \" \" + column,\n",
    "                    tokens,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_total_length,\n",
    "                    truncation_strategy=\"longest_first\",\n",
    "                    truncation=True,\n",
    "                )\n",
    "\n",
    "            input_ids = tokenize_result[\"input_ids\"]\n",
    "            input_mask = tokenize_result[\"attention_mask\"]\n",
    "\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            column_token_length = 0\n",
    "            if self.config[\"base_class\"] == \"roberta\":\n",
    "                for i, token_id in enumerate(input_ids):\n",
    "                    if token_id == self.tokenizer.sep_token_id:\n",
    "                        column_token_length = i + 2\n",
    "                        break\n",
    "                segment_ids = [0] * max_total_length\n",
    "                for i in range(column_token_length, max_total_length):\n",
    "                    if input_mask[i] == 0:\n",
    "                        break\n",
    "                    segment_ids[i] = 1\n",
    "            else:\n",
    "                for i, token_id in enumerate(input_ids):\n",
    "                    if token_id == self.tokenizer.sep_token_id:\n",
    "                        column_token_length = i + 1\n",
    "                        break\n",
    "                segment_ids = tokenize_result[\"token_type_ids\"]\n",
    "\n",
    "            subword_to_word = [0] * column_token_length + subword_to_word\n",
    "            word_to_subword = [(pos[0]+column_token_length, pos[1]+column_token_length) for pos in word_to_subword]\n",
    "\n",
    "            assert len(input_ids) == max_total_length\n",
    "            assert len(input_mask) == max_total_length\n",
    "            assert len(segment_ids) == max_total_length\n",
    "\n",
    "            input_feature.tokens.append(tokens)\n",
    "            input_feature.word_to_subword.append(word_to_subword)\n",
    "            input_feature.subword_to_word.append(subword_to_word)\n",
    "            input_feature.input_ids.append(input_ids)\n",
    "            input_feature.input_mask.append(input_mask)\n",
    "            input_feature.segment_ids.append(segment_ids)\n",
    "\n",
    "        return input_feature\n",
    "\n",
    "    def fill_label_feature(self, example: SQLExample, input_feature: InputFeature, config):\n",
    "        max_total_length = int(config[\"max_total_length\"])\n",
    "\n",
    "        columns = [c[0] for c in example.column_meta]\n",
    "        col_num = len(columns)\n",
    "        input_feature.columns = columns\n",
    "\n",
    "        input_feature.agg = [0] * col_num\n",
    "        input_feature.agg[example.select] = example.agg\n",
    "        input_feature.where_num = [len(example.conditions)] * col_num\n",
    "\n",
    "        input_feature.select = [0] * len(columns)\n",
    "        input_feature.select[example.select] = 1\n",
    "\n",
    "        input_feature.where = [0] * len(columns)\n",
    "        input_feature.op = [0] * len(columns)\n",
    "        input_feature.value_start = [0] * len(columns)\n",
    "        input_feature.value_end = [0] * len(columns)\n",
    "\n",
    "        for colidx, op, _ in example.conditions:\n",
    "            input_feature.where[colidx] = 1\n",
    "            input_feature.op[colidx] = op\n",
    "        for colidx, column_meta in enumerate(example.column_meta):\n",
    "            if column_meta[-1] == None:\n",
    "                continue\n",
    "            se = example.value_start_end[column_meta[-1]]\n",
    "            try:\n",
    "                s = input_feature.word_to_subword[colidx][se[0]][0]\n",
    "                input_feature.value_start[colidx] = s\n",
    "                e = input_feature.word_to_subword[colidx][se[1]-1][1]-1\n",
    "                input_feature.value_end[colidx] = e\n",
    "                assert s < max_total_length and input_feature.input_mask[colidx][s] == 1\n",
    "                assert e < max_total_length and input_feature.input_mask[colidx][e] == 1\n",
    "\n",
    "            except:\n",
    "                print(\"value span is out of range\")\n",
    "                return False\n",
    "\n",
    "        # feature_sq = input_feature.output_SQ(return_str=False)\n",
    "        # example_sq = example.output_SQ(return_str=False)\n",
    "        # if feature_sq != example_sq:\n",
    "        #     print(example.qid, feature_sq, example_sq)\n",
    "        return True\n",
    "\n",
    "    def load_data(self, data_paths, config, include_label=False):\n",
    "        model_inputs = {k: [] for k in [\"input_ids\", \"input_mask\", \"segment_ids\"]}\n",
    "        if include_label:\n",
    "            for k in [\"agg\", \"select\", \"where_num\", \"where\", \"op\", \"value_start\", \"value_end\"]:\n",
    "                model_inputs[k] = []\n",
    "\n",
    "        pos = []\n",
    "        input_features = []\n",
    "        for data_path in data_paths.split(\"|\"):\n",
    "            cnt = 0\n",
    "            for line in open(data_path, encoding=\"utf8\"):\n",
    "                example = SQLExample.load_from_json(line)\n",
    "                if not example.valid and include_label == True:\n",
    "                    continue\n",
    "\n",
    "                input_feature = self.get_input_feature(example, config)\n",
    "                if include_label:\n",
    "                    success = self.fill_label_feature(example, input_feature, config)\n",
    "                    if not success:\n",
    "                        continue\n",
    "\n",
    "                # sq = input_feature.output_SQ()\n",
    "                input_features.append(input_feature)\n",
    "\n",
    "                cur_start = len(model_inputs[\"input_ids\"])\n",
    "                cur_sample_num = len(input_feature.input_ids)\n",
    "                pos.append((cur_start, cur_start + cur_sample_num))\n",
    "\n",
    "                model_inputs[\"input_ids\"].extend(input_feature.input_ids)\n",
    "                model_inputs[\"input_mask\"].extend(input_feature.input_mask)\n",
    "                model_inputs[\"segment_ids\"].extend(input_feature.segment_ids)\n",
    "                if include_label:\n",
    "                    model_inputs[\"agg\"].extend(input_feature.agg)\n",
    "                    model_inputs[\"select\"].extend(input_feature.select)\n",
    "                    model_inputs[\"where_num\"].extend(input_feature.where_num)\n",
    "                    model_inputs[\"where\"].extend(input_feature.where)\n",
    "                    model_inputs[\"op\"].extend(input_feature.op)\n",
    "                    model_inputs[\"value_start\"].extend(input_feature.value_start)\n",
    "                    model_inputs[\"value_end\"].extend(input_feature.value_end)\n",
    "\n",
    "                cnt += 1\n",
    "                if cnt % 5000 == 0:\n",
    "                    print(cnt)\n",
    "\n",
    "                if \"DEBUG\" in config and cnt > 100:\n",
    "                    break\n",
    "\n",
    "        for k in model_inputs:\n",
    "            model_inputs[k] = np.array(model_inputs[k], dtype=np.int64)\n",
    "\n",
    "        return input_features, model_inputs, pos\n",
    "\n",
    "class SQLDataset(torch_data.Dataset):\n",
    "    def __init__(self, data_paths, config, featurizer, include_label=False):\n",
    "        self.config = config\n",
    "        self.featurizer = featurizer\n",
    "        self.input_features, self.model_inputs, self.pos = self.featurizer.load_data(data_paths, config, include_label)\n",
    "\n",
    "        print(\"{0} loaded. Data shapes:\".format(data_paths))\n",
    "        for k, v in self.model_inputs.items():\n",
    "            print(k, v.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.model_inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.model_inputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tulip/Downloads/cs626/Text2SQL/colab_data/data/wikitrain.jsonl loaded. Data shapes:\n",
      "input_ids (14537, 96)\n",
      "input_mask (14537, 96)\n",
      "segment_ids (14537, 96)\n",
      "agg (14537,)\n",
      "select (14537,)\n",
      "where_num (14537,)\n",
      "where (14537,)\n",
      "op (14537,)\n",
      "value_start (14537,)\n",
      "value_end (14537,)\n"
     ]
    }
   ],
   "source": [
    "# Create training data (i/o)\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import datetime\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "config = read_conf(\"wikisql.conf\")\n",
    "\n",
    "featurizer = HydraFeaturizer(config)\n",
    "train_data = SQLDataset(config[\"train_data_path\"], config, featurizer, True)\n",
    "train_data_loader = torch_data.DataLoader(train_data, batch_size=int(config[\"batch_size\"]), shuffle=True, pin_memory=True)\n",
    "\n",
    "num_samples = len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "class BaseModel(object):\n",
    "    \"\"\"Define common interfaces for HydraNet models\"\"\"\n",
    "    def train_on_batch(self, batch):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def save(self, model_path, epoch):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def load(self, model_path, epoch):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def model_inference(self, model_inputs):\n",
    "        \"\"\"model prediction on processed features\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def dataset_inference(self, dataset: SQLDataset):\n",
    "        print(\"model prediction start\")\n",
    "        start_time = time.time()\n",
    "        model_outputs = self.model_inference(dataset.model_inputs)\n",
    "\n",
    "        final_outputs = []\n",
    "        for pos in dataset.pos:\n",
    "            final_output = {}\n",
    "            for k in model_outputs:\n",
    "                final_output[k] = model_outputs[k][pos[0]:pos[1], :]\n",
    "            final_outputs.append(final_output)\n",
    "        print(\"model prediction end, time elapse: {0}\".format(time.time() - start_time))\n",
    "        assert len(dataset.input_features) == len(final_outputs)\n",
    "\n",
    "        return final_outputs\n",
    "\n",
    "    def predict_SQL(self, dataset: SQLDataset, model_outputs=None):\n",
    "        if model_outputs is None:\n",
    "            model_outputs = self.dataset_inference(dataset)\n",
    "        sqls = []\n",
    "        for input_feature, model_output in zip(dataset.input_features, model_outputs):\n",
    "            agg, select, where, conditions = self.parse_output(input_feature, model_output, [])\n",
    "\n",
    "            conditions_with_value_texts = []\n",
    "            for wc in where:\n",
    "                _, op, vs, ve = conditions[wc]\n",
    "                word_start, word_end = input_feature.subword_to_word[wc][vs], input_feature.subword_to_word[wc][ve]\n",
    "                char_start = input_feature.word_to_char_start[word_start]\n",
    "                char_end = len(input_feature.question)\n",
    "                if word_end + 1 < len(input_feature.word_to_char_start):\n",
    "                    char_end = input_feature.word_to_char_start[word_end + 1]\n",
    "                value_span_text = input_feature.question[char_start:char_end].rstrip()\n",
    "                conditions_with_value_texts.append((wc, op, value_span_text))\n",
    "\n",
    "            sqls.append((agg, select, conditions_with_value_texts))\n",
    "\n",
    "        return sqls\n",
    "\n",
    "    def predict_SQL_with_EG(self, engine, dataset: SQLDataset, beam_size=5, model_outputs=None):\n",
    "        if model_outputs is None:\n",
    "            model_outputs = self.dataset_inference(dataset)\n",
    "        sqls = []\n",
    "        for input_feature, model_output in zip(dataset.input_features, model_outputs):\n",
    "            agg, select, where_num, conditions = self.beam_parse_output(input_feature, model_output, beam_size)\n",
    "            query = {\"agg\": agg, \"sel\": select, \"conds\": []}\n",
    "            wcs = set()\n",
    "            conditions_with_value_texts = []\n",
    "            for condition in conditions:\n",
    "                if len(wcs) >= where_num:\n",
    "                    break\n",
    "                _, wc, op, vs, ve = condition\n",
    "                if wc in wcs:\n",
    "                    continue\n",
    "\n",
    "                word_start, word_end = input_feature.subword_to_word[wc][vs], input_feature.subword_to_word[wc][ve]\n",
    "                char_start = input_feature.word_to_char_start[word_start]\n",
    "                char_end = len(input_feature.question)\n",
    "                if word_end + 1 < len(input_feature.word_to_char_start):\n",
    "                    char_end = input_feature.word_to_char_start[word_end + 1]\n",
    "                value_span_text = input_feature.question[char_start:char_end].rstrip()\n",
    "\n",
    "                query[\"conds\"] = [[int(wc), int(op), value_span_text]]\n",
    "                result, sql = engine.execute_dict_query(input_feature.table_id, query)\n",
    "                if not result or 'ERROR: ' in result:\n",
    "                    continue\n",
    "\n",
    "                conditions_with_value_texts.append((wc, op, value_span_text))\n",
    "                wcs.add(wc)\n",
    "\n",
    "            sqls.append((agg, select, conditions_with_value_texts))\n",
    "\n",
    "        return sqls\n",
    "\n",
    "    def _get_where_num(self, output):\n",
    "        # wn = np.argmax(output[\"where_num\"], -1)\n",
    "        # max_num = 0\n",
    "        # max_cnt = np.sum(wn == 0)\n",
    "        # for num in range(1, 5):\n",
    "        #     cur_cnt = np.sum(wn==num)\n",
    "        #     if cur_cnt > max_cnt:\n",
    "        #         max_cnt = cur_cnt\n",
    "        #         max_num = num\n",
    "        # def sigmoid(x):\n",
    "        #     return 1/(1 + np.exp(-x))\n",
    "        relevant_prob = 1 - np.exp(output[\"column_func\"][:, 2])\n",
    "        where_num_scores = np.average(output[\"where_num\"], axis=0, weights=relevant_prob)\n",
    "        where_num = int(np.argmax(where_num_scores))\n",
    "\n",
    "        return where_num\n",
    "\n",
    "    def parse_output(self, input_feature, model_output, where_label = []):\n",
    "        def get_span(i):\n",
    "            offset = 0\n",
    "            segment_ids = np.array(input_feature.segment_ids[i])\n",
    "            for j in range(len(segment_ids)):\n",
    "                if segment_ids[j] == 1:\n",
    "                    offset = j\n",
    "                    break\n",
    "\n",
    "            value_start, value_end = model_output[\"value_start\"][i, segment_ids == 1], model_output[\"value_end\"][i, segment_ids == 1]\n",
    "            l = len(value_start)\n",
    "            sum_mat = value_start.reshape((l, 1)) + value_end.reshape((1, l))\n",
    "            span = (0, 0)\n",
    "            for cur_span, _ in sorted(np.ndenumerate(sum_mat), key=lambda x:x[1], reverse=True):\n",
    "                if cur_span[1] < cur_span[0] or cur_span[0] == l - 1 or cur_span[1] == l - 1:\n",
    "                    continue\n",
    "                span = cur_span\n",
    "                break\n",
    "\n",
    "            return (span[0]+offset, span[1]+offset)\n",
    "\n",
    "        select_id_prob = sorted(enumerate(model_output[\"column_func\"][:, 0]), key=lambda x:x[1], reverse=True)\n",
    "        select = select_id_prob[0][0]\n",
    "        agg = np.argmax(model_output[\"agg\"][select, :])\n",
    "\n",
    "        where_id_prob = sorted(enumerate(model_output[\"column_func\"][:, 1]), key=lambda x:x[1], reverse=True)\n",
    "        where_num = self._get_where_num(model_output)\n",
    "        where = [i for i, _ in where_id_prob[:where_num]]\n",
    "        conditions = {}\n",
    "        for idx in set(where + where_label):\n",
    "            span = get_span(idx)\n",
    "            op = np.argmax(model_output[\"op\"][idx, :])\n",
    "            conditions[idx] = (idx, op, span[0], span[1])\n",
    "\n",
    "        return agg, select, where, conditions\n",
    "\n",
    "    def beam_parse_output(self, input_feature, model_output, beam_size=5):\n",
    "        def get_span(i):\n",
    "            offset = 0\n",
    "            segment_ids = np.array(input_feature.segment_ids[i])\n",
    "            for j in range(len(segment_ids)):\n",
    "                if segment_ids[j] == 1:\n",
    "                    offset = j\n",
    "                    break\n",
    "\n",
    "            value_start, value_end = model_output[\"value_start\"][i, segment_ids == 1], model_output[\"value_end\"][i, segment_ids == 1]\n",
    "            l = len(value_start)\n",
    "            sum_mat = value_start.reshape((l, 1)) + value_end.reshape((1, l))\n",
    "            spans = []\n",
    "            for cur_span, sum_logp in sorted(np.ndenumerate(sum_mat), key=lambda x:x[1], reverse=True):\n",
    "                if cur_span[1] < cur_span[0] or cur_span[0] == l - 1 or cur_span[1] == l - 1:\n",
    "                    continue\n",
    "                spans.append((cur_span[0]+offset, cur_span[1]+offset, sum_logp))\n",
    "                if len(spans) >= beam_size:\n",
    "                    break\n",
    "\n",
    "            return spans\n",
    "\n",
    "        select_id_prob = sorted(enumerate(model_output[\"column_func\"][:, 0]), key=lambda x:x[1], reverse=True)\n",
    "        select = select_id_prob[0][0]\n",
    "        agg = np.argmax(model_output[\"agg\"][select, :])\n",
    "\n",
    "        where_id_prob = sorted(enumerate(model_output[\"column_func\"][:, 1]), key=lambda x:x[1], reverse=True)\n",
    "        where_num = self._get_where_num(model_output)\n",
    "        conditions = []\n",
    "        for idx, wlogp in where_id_prob[:beam_size]:\n",
    "            op = np.argmax(model_output[\"op\"][idx, :])\n",
    "            for span in get_span(idx):\n",
    "                conditions.append((wlogp+span[2], idx, op, span[0], span[1]))\n",
    "        conditions.sort(key=lambda x:x[0], reverse=True)\n",
    "        return agg, select, where_num, conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch model\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import transformers\n",
    "from torch import nn\n",
    "\n",
    "class HydraTorch(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = HydraNet(config)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer, self.scheduler = None, None\n",
    "\n",
    "    def train_on_batch(self, batch):\n",
    "        if self.optimizer is None:\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": float(self.config[\"decay\"]),\n",
    "                },\n",
    "                {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                 \"weight_decay\": 0.0},\n",
    "            ]\n",
    "            self.optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=float(self.config[\"learning_rate\"]))\n",
    "            self.scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=int(self.config[\"num_warmup_steps\"]),\n",
    "                num_training_steps=int(self.config[\"num_train_steps\"]))\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        self.model.train()\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(self.device)\n",
    "        batch_loss = torch.mean(self.model(**batch)[\"loss\"])\n",
    "        batch_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return batch_loss.cpu().detach().numpy()\n",
    "\n",
    "    def model_inference(self, model_inputs):\n",
    "        self.model.eval()\n",
    "        model_outputs = {}\n",
    "        batch_size = 512\n",
    "        for start_idx in range(0, model_inputs[\"input_ids\"].shape[0], batch_size):\n",
    "            input_tensor = {k: torch.from_numpy(model_inputs[k][start_idx:start_idx+batch_size]).to(self.device) for k in [\"input_ids\", \"input_mask\", \"segment_ids\"]}\n",
    "            with torch.no_grad():\n",
    "                model_output = self.model(**input_tensor)\n",
    "            for k, out_tensor in model_output.items():\n",
    "                if out_tensor is None:\n",
    "                    continue\n",
    "                if k not in model_outputs:\n",
    "                    model_outputs[k] = []\n",
    "                model_outputs[k].append(out_tensor.cpu().detach().numpy())\n",
    "\n",
    "        for k in model_outputs:\n",
    "            model_outputs[k] = np.concatenate(model_outputs[k], 0)\n",
    "\n",
    "        return model_outputs\n",
    "\n",
    "    def save(self, model_path, epoch):\n",
    "        if \"SAVE\" in self.config and \"DEBUG\" not in self.config:\n",
    "            save_path = os.path.join(model_path, \"model_{0}.pt\".format(epoch))\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                torch.save(self.model.module.state_dict(), save_path)\n",
    "            else:\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "    def load(self, model_path, epoch):\n",
    "        pt_path = os.path.join(model_path, \"model_{0}.pt\".format(epoch))\n",
    "        loaded_dict = torch.load(pt_path, map_location=torch.device(self.device))\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.model.module.load_state_dict(loaded_dict)\n",
    "        else:\n",
    "            self.model.load_state_dict(loaded_dict)\n",
    "        print(\"PyTorch model loaded from {0}\".format(pt_path))\n",
    "\n",
    "class HydraNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(HydraNet, self).__init__()\n",
    "        self.config = config\n",
    "        self.base_model = create_base_model(config)\n",
    "\n",
    "        # #=====Hack for RoBERTa model====\n",
    "        # self.base_model.config.type_vocab_size = 2\n",
    "        # single_emb = self.base_model.embeddings.token_type_embeddings\n",
    "        # self.base_model.embeddings.token_type_embeddings = torch.nn.Embedding(2, single_emb.embedding_dim)\n",
    "        # self.base_model.embeddings.token_type_embeddings.weight = torch.nn.Parameter(single_emb.weight.repeat([2, 1]), requires_grad=True)\n",
    "        # #====================================\n",
    "\n",
    "        drop_rate = float(config[\"drop_rate\"]) if \"drop_rate\" in config else 0.0\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "        bert_hid_size = self.base_model.config.hidden_size\n",
    "        self.column_func = nn.Linear(bert_hid_size, 3)\n",
    "        self.agg = nn.Linear(bert_hid_size, int(config[\"agg_num\"]))\n",
    "        self.op = nn.Linear(bert_hid_size, int(config[\"op_num\"]))\n",
    "        self.where_num = nn.Linear(bert_hid_size, int(config[\"where_column_num\"]) + 1)\n",
    "        self.start_end = nn.Linear(bert_hid_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, input_mask, segment_ids, agg=None, select=None, where=None, where_num=None, op=None, value_start=None, value_end=None):\n",
    "        # print(\"[inner] input_ids size:\", input_ids.size())\n",
    "        if self.config[\"base_class\"] == \"roberta\":\n",
    "            bert_output, pooled_output = self.base_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=input_mask,\n",
    "                token_type_ids=None,\n",
    "                return_dict=False)\n",
    "        else:\n",
    "            bert_output, pooled_output = self.base_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=input_mask,\n",
    "                token_type_ids=segment_ids,\n",
    "                return_dict=False)\n",
    "\n",
    "        bert_output = self.dropout(bert_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        column_func_logit = self.column_func(pooled_output)\n",
    "        agg_logit = self.agg(pooled_output)\n",
    "        op_logit = self.op(pooled_output)\n",
    "        where_num_logit = self.where_num(pooled_output)\n",
    "        start_end_logit = self.start_end(bert_output)\n",
    "        value_span_mask = input_mask.to(dtype=bert_output.dtype)\n",
    "        # value_span_mask[:, 0] = 1\n",
    "        start_logit = start_end_logit[:, :, 0] * value_span_mask - 1000000.0 * (1 - value_span_mask)\n",
    "        end_logit = start_end_logit[:, :, 1] * value_span_mask - 1000000.0 * (1 - value_span_mask)\n",
    "\n",
    "        loss = None\n",
    "        if select is not None:\n",
    "            bceloss = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "            cross_entropy = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "            loss = cross_entropy(agg_logit, agg) * select.float()\n",
    "            loss += bceloss(column_func_logit[:, 0], select.float())\n",
    "            loss += bceloss(column_func_logit[:, 1], where.float())\n",
    "            loss += bceloss(column_func_logit[:, 2], (1-select.float()) * (1-where.float()))\n",
    "            loss += cross_entropy(where_num_logit, where_num)\n",
    "            loss += cross_entropy(op_logit, op) * where.float()\n",
    "            loss += cross_entropy(start_logit, value_start)\n",
    "            loss += cross_entropy(end_logit, value_end)\n",
    "\n",
    "\n",
    "        # return loss, column_func_logit, agg_logit, op_logit, where_num_logit, start_logit, end_logit\n",
    "        log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "        return {\"column_func\": log_sigmoid(column_func_logit),\n",
    "                \"agg\": agg_logit.log_softmax(1),\n",
    "                \"op\": op_logit.log_softmax(1),\n",
    "                \"where_num\": where_num_logit.log_softmax(1),\n",
    "                \"value_start\": start_logit.log_softmax(1),\n",
    "                \"value_end\": end_logit.log_softmax(1),\n",
    "                \"loss\": loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config, is_train = False) -> BaseModel:\n",
    "    if config[\"model_type\"] == \"pytorch\":\n",
    "        return HydraTorch(config)\n",
    "    # elif config[\"model_type\"] == \"tf\":\n",
    "    #     return HydraTensorFlow(config, is_train, num_gpu)\n",
    "    else:\n",
    "        raise NotImplementedError(\"model type {0} is not supported\".format(config[\"model_type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tulip/Downloads/cs626/Text2SQL/colab_data\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class HydraEvaluator():\n",
    "    def __init__(self, output_path, config, hydra_featurizer: HydraFeaturizer, model:BaseModel, note=\"\"):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.eval_history_file = os.path.join(output_path, \"eval.log\")\n",
    "        self.bad_case_dir = os.path.join(output_path, \"bad_cases\")\n",
    "        os.mkdir(output_path)\n",
    "        if \"DEBUG\" not in config:\n",
    "            os.mkdir(self.bad_case_dir)\n",
    "            with open(self.eval_history_file, \"w\", encoding=\"utf8\") as f:\n",
    "                f.write(note.rstrip() + \"\\n\")\n",
    "\n",
    "        self.eval_data = {}\n",
    "        for eval_path in config[\"dev_data_path\"].split(\"|\") + config[\"test_data_path\"].split(\"|\"):\n",
    "            eval_data = SQLDataset(eval_path, config, hydra_featurizer, True)\n",
    "            self.eval_data[os.path.basename(eval_path)] = eval_data\n",
    "\n",
    "            print(\"Eval Data file {0} loaded, sample num = {1}\".format(eval_path, len(eval_data)))\n",
    "\n",
    "    def _eval_imp(self, eval_data: SQLDataset, get_sq=True):\n",
    "        items = [\"overall\", \"agg\", \"sel\", \"wn\", \"wc\", \"op\", \"val\"]\n",
    "        acc = {k:0.0 for k in items}\n",
    "        sq = []\n",
    "        cnt = 0\n",
    "        model_outputs = self.model.dataset_inference(eval_data)\n",
    "        for input_feature, model_output in zip(eval_data.input_features, model_outputs):\n",
    "            cur_acc = {k:1 for k in acc if k != \"overall\"}\n",
    "\n",
    "            select_label = np.argmax(input_feature.select)\n",
    "            agg_label = input_feature.agg[select_label]\n",
    "            wn_label = input_feature.where_num[0]\n",
    "            wc_label = [i for i, w in enumerate(input_feature.where) if w == 1]\n",
    "\n",
    "            agg, select, where, conditions = self.model.parse_output(input_feature, model_output, wc_label)\n",
    "            if agg != agg_label:\n",
    "                cur_acc[\"agg\"] = 0\n",
    "            if select != select_label:\n",
    "                cur_acc[\"sel\"] = 0\n",
    "            if len(where) != wn_label:\n",
    "                cur_acc[\"wn\"] = 0\n",
    "            if set(where) != set(wc_label):\n",
    "                cur_acc[\"wc\"] = 0\n",
    "\n",
    "            for w in wc_label:\n",
    "                _, op, vs, ve = conditions[w]\n",
    "                if op != input_feature.op[w]:\n",
    "                    cur_acc[\"op\"] = 0\n",
    "\n",
    "                if vs != input_feature.value_start[w] or ve != input_feature.value_end[w]:\n",
    "                    cur_acc[\"val\"] = 0\n",
    "\n",
    "            for k in cur_acc:\n",
    "                acc[k] += cur_acc[k]\n",
    "\n",
    "            all_correct = 0 if 0 in cur_acc.values() else 1\n",
    "            acc[\"overall\"] += all_correct\n",
    "\n",
    "            if (\"DEBUG\" in self.config or get_sq) and not all_correct:\n",
    "                try:\n",
    "                    true_sq = input_feature.output_SQ()\n",
    "                    pred_sq = input_feature.output_SQ(agg=agg, sel=select, conditions=[conditions[w] for w in where])\n",
    "                    task_cor_text = \"\".join([str(cur_acc[k]) for k in items if k in cur_acc])\n",
    "                    sq.append([str(cnt), input_feature.question, \"|\".join([task_cor_text, pred_sq, true_sq])])\n",
    "                except:\n",
    "                    pass\n",
    "            cnt += 1\n",
    "\n",
    "        result_str = []\n",
    "        for item in items:\n",
    "            result_str.append(item + \":{0:.1f}\".format(acc[item] * 100.0 / cnt))\n",
    "\n",
    "        result_str = \", \".join(result_str)\n",
    "\n",
    "        return result_str, sq\n",
    "\n",
    "    def eval(self, epochs):\n",
    "        print(self.bad_case_dir)\n",
    "        for eval_file in self.eval_data:\n",
    "            result_str, sq = self._eval_imp(self.eval_data[eval_file])\n",
    "            print(eval_file + \": \" + result_str)\n",
    "\n",
    "            if \"DEBUG\" in self.config:\n",
    "                for text in sq:\n",
    "                    print(text[0] + \":\" + text[1] + \"\\t\" + text[2])\n",
    "            else:\n",
    "                with open(self.eval_history_file, \"a+\", encoding=\"utf8\") as f:\n",
    "                    f.write(\"[{0}, epoch {1}] \".format(eval_file, epochs) + result_str + \"\\n\")\n",
    "\n",
    "                bad_case_file = os.path.join(self.bad_case_dir,\n",
    "                                           \"{0}_epoch_{1}.log\".format(eval_file, epochs))\n",
    "                with open(bad_case_file, \"w\", encoding=\"utf8\") as f:\n",
    "                    for text in sq:\n",
    "                        f.write(text[0] + \":\" + text[1] + \"\\t\" + text[2] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps: 1135, warm_up_steps: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tulip/Downloads/cs626/Text2SQL/colab_data/data/wikidev.jsonl loaded. Data shapes:\n",
      "input_ids (14537, 96)\n",
      "input_mask (14537, 96)\n",
      "segment_ids (14537, 96)\n",
      "agg (14537,)\n",
      "select (14537,)\n",
      "where_num (14537,)\n",
      "where (14537,)\n",
      "op (14537,)\n",
      "value_start (14537,)\n",
      "value_end (14537,)\n",
      "Eval Data file /Users/tulip/Downloads/cs626/Text2SQL/colab_data/data/wikidev.jsonl loaded, sample num = 14537\n",
      "data/wikitest.jsonl loaded. Data shapes:\n",
      "input_ids (14431, 96)\n",
      "input_mask (14431, 96)\n",
      "segment_ids (14431, 96)\n",
      "agg (14431,)\n",
      "select (14431,)\n",
      "where_num (14431,)\n",
      "where (14431,)\n",
      "op (14431,)\n",
      "value_start (14431,)\n",
      "value_end (14431,)\n",
      "Eval Data file data/wikitest.jsonl loaded, sample num = 14431\n"
     ]
    }
   ],
   "source": [
    "note = \"\"\n",
    "model_path = \"model--\"+datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "config[\"num_train_steps\"] = int(num_samples * int(config[\"epochs\"]) / int(config[\"batch_size\"]))\n",
    "step_per_epoch = num_samples / int(config[\"batch_size\"])\n",
    "print(\"total_steps: {0}, warm_up_steps: {1}\".format(config[\"num_train_steps\"], config[\"num_warmup_steps\"]))\n",
    "\n",
    "model = create_model(config, is_train=True)\n",
    "evaluator = HydraEvaluator(model_path, config, featurizer, model, note)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "model--20221106_184457\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "[11-06 18:46:06] epoch 0, batch 0, batch_loss=10.9957\n",
      "1\n",
      "here1\n",
      "here2\n",
      "[11-06 18:46:56] epoch 0, batch 1, batch_loss=10.4349\n",
      "2\n",
      "here1\n",
      "here2\n",
      "[11-06 18:47:46] epoch 0, batch 2, batch_loss=10.3400\n",
      "3\n",
      "here1\n",
      "here2\n",
      "[11-06 18:48:39] epoch 0, batch 3, batch_loss=10.5771\n",
      "4\n",
      "here1\n",
      "here2\n",
      "[11-06 18:49:37] epoch 0, batch 4, batch_loss=10.5017\n",
      "5\n",
      "here1\n",
      "here2\n",
      "[11-06 18:50:37] epoch 0, batch 5, batch_loss=10.6195\n",
      "6\n",
      "here1\n",
      "here2\n",
      "[11-06 18:51:37] epoch 0, batch 6, batch_loss=10.5427\n",
      "7\n",
      "here1\n",
      "here2\n",
      "[11-06 18:52:40] epoch 0, batch 7, batch_loss=10.5049\n",
      "8\n",
      "here1\n",
      "here2\n",
      "[11-06 18:53:47] epoch 0, batch 8, batch_loss=10.4735\n",
      "9\n",
      "here1\n",
      "here2\n",
      "[11-06 18:54:44] epoch 0, batch 9, batch_loss=10.3813\n",
      "10\n",
      "here1\n",
      "here2\n",
      "[11-06 18:55:40] epoch 0, batch 10, batch_loss=10.6668\n",
      "11\n",
      "here1\n",
      "here2\n",
      "[11-06 18:56:39] epoch 0, batch 11, batch_loss=10.4802\n",
      "12\n",
      "here1\n",
      "here2\n",
      "[11-06 18:57:38] epoch 0, batch 12, batch_loss=10.3270\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(\"start training\")\n",
    "print(model_path)\n",
    "loss_avg, step, epoch = 0.0, 0, 0\n",
    "while True:\n",
    "    for batch_id, batch in enumerate(train_data_loader):\n",
    "        print(batch_id)\n",
    "        cur_loss = model.train_on_batch(batch)\n",
    "        loss_avg = (loss_avg * step + cur_loss) / (step + 1)\n",
    "        step += 1\n",
    "        print(\"here1\")\n",
    "        if batch_id % 1 == 0:\n",
    "            print(\"here2\")\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print(\"[{3}] epoch {0}, batch {1}, batch_loss={2:.4f}\".format(epoch, batch_id, cur_loss,\n",
    "                                                                            currentDT.strftime(\"%m-%d %H:%M:%S\")))\n",
    "    model.save(model_path, epoch)\n",
    "    evaluator.eval(epoch)\n",
    "    epoch += 1\n",
    "    if epoch >= int(config[\"epochs\"]):\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
