{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class str2(str):\n",
    "    def __repr__(self):\n",
    "        return ''.join(('\"', super().__repr__()[1:-1], '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/muril-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['कोई', 'अच्छी', 'सी']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# text = [str2(token) for token in text]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     14\u001b[0m input_encoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2259\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2221'>2222</a>\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2222'>2223</a>\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2223'>2224</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2241'>2242</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2242'>2243</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2243'>2244</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2244'>2245</a>\u001b[0m \u001b[39m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2245'>2246</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2256'>2257</a>\u001b[0m \u001b[39m            method).\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2257'>2258</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2258'>2259</a>\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2259'>2260</a>\u001b[0m         text,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2260'>2261</a>\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2261'>2262</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2262'>2263</a>\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2263'>2264</a>\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2264'>2265</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2265'>2266</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2266'>2267</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2267'>2268</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2268'>2269</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2270'>2271</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2667\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2656'>2657</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2657'>2658</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2658'>2659</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2659'>2660</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2663'>2664</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2664'>2665</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2666'>2667</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2667'>2668</a>\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2668'>2669</a>\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2669'>2670</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2670'>2671</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2671'>2672</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2672'>2673</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2673'>2674</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2674'>2675</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2675'>2676</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2676'>2677</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2677'>2678</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2678'>2679</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2679'>2680</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2680'>2681</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2681'>2682</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2682'>2683</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2683'>2684</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2684'>2685</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2685'>2686</a>\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:502\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=478'>479</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=479'>480</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=480'>481</a>\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=497'>498</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=498'>499</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=500'>501</a>\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=501'>502</a>\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=502'>503</a>\u001b[0m         batched_input,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=503'>504</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=504'>505</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=505'>506</a>\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=506'>507</a>\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=507'>508</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=508'>509</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=509'>510</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=510'>511</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=511'>512</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=512'>513</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=513'>514</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=514'>515</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=515'>516</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=516'>517</a>\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=517'>518</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=518'>519</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=519'>520</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=521'>522</a>\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=522'>523</a>\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=523'>524</a>\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=419'>420</a>\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=420'>421</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=421'>422</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=422'>423</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=425'>426</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=426'>427</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=428'>429</a>\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=429'>430</a>\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=430'>431</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=431'>432</a>\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=432'>433</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=434'>435</a>\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=435'>436</a>\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=436'>437</a>\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=437'>438</a>\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=438'>439</a>\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=439'>440</a>\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=440'>441</a>\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=441'>442</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=442'>443</a>\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=451'>452</a>\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=452'>453</a>\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "path = 'google/muril-base-cased'\n",
    "## Loading the model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path,output_hidden_states=True)\n",
    "## Embeddings \n",
    "# text = [\"कोई अच्छी सी फिल्म लगायो\", \"कोई\"]\n",
    "text = ['कोई', 'अच्छी', 'सी']\n",
    "# text = [str2(token) for token in text]\n",
    "print(text)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(text)))\n",
    "input_encoded = tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "         states = model(**input_encoded).hidden_states\n",
    "output = torch.stack([states[i] for i in range(len(states))])\n",
    "output = output.squeeze()\n",
    "print(\"Output shape is {}\".format(output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class InputFeature(object):\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 table_id,\n",
    "                 tokens,\n",
    "                 word_to_char_start,\n",
    "                 word_to_subword,\n",
    "                 subword_to_word,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids):\n",
    "        self.question = question\n",
    "        self.table_id = table_id\n",
    "        self.tokens = tokens\n",
    "        self.word_to_char_start = word_to_char_start\n",
    "        self.word_to_subword = word_to_subword\n",
    "        self.subword_to_word = subword_to_word\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "\n",
    "        self.columns = None\n",
    "        self.agg = None\n",
    "        self.select = None\n",
    "        self.where_num = None\n",
    "        self.where = None\n",
    "        self.op = None\n",
    "        self.value_start = None\n",
    "        self.value_end = None\n",
    "\n",
    "    def output_SQ(self, agg = None, sel = None, conditions = None, return_str=True):\n",
    "        agg_ops = ['NA', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "        cond_ops = ['=', '>', '<', 'OP']\n",
    "\n",
    "        if agg is None and sel is None and conditions is None:\n",
    "            sel = np.argmax(self.select)\n",
    "            agg = self.agg[sel]\n",
    "            conditions = []\n",
    "            for i in range(len(self.where)):\n",
    "                if self.where[i] == 0:\n",
    "                    continue\n",
    "                conditions.append((i, self.op[i], self.value_start[i], self.value_end[i]))\n",
    "\n",
    "        agg_text = agg_ops[agg]\n",
    "        select_text = self.columns[sel]\n",
    "        cond_texts = []\n",
    "        for wc, op, vs, ve in conditions:\n",
    "            column_text = self.columns[wc]\n",
    "            op_text = cond_ops[op]\n",
    "            word_start, word_end = self.subword_to_word[wc][vs], self.subword_to_word[wc][ve]\n",
    "            char_start = self.word_to_char_start[word_start]\n",
    "            char_end = len(self.question) if word_end + 1 >= len(self.word_to_char_start) else self.word_to_char_start[word_end + 1]\n",
    "            value_span_text = self.question[char_start:char_end]\n",
    "            cond_texts.append(column_text + op_text + value_span_text.rstrip())\n",
    "\n",
    "        if return_str:\n",
    "            sq = agg_text + \", \" + select_text + \", \" + \" AND \".join(cond_texts)\n",
    "        else:\n",
    "            sq = (agg_text, select_text, set(cond_texts))\n",
    "\n",
    "        return sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/muril-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player\n",
      "string\n",
      "['कोई', 'अच्छी', 'सी']\n",
      "No.\n",
      "string\n",
      "['कोई', 'अच्छी', 'सी']\n",
      "Nationality\n",
      "string\n",
      "['कोई', 'अच्छी', 'सी']\n",
      "Position\n",
      "string\n",
      "['कोई', 'अच्छी', 'सी']\n",
      "Years in Toronto\n",
      "string\n",
      "['कोई', 'अच्छी', 'सी']\n",
      "School/Club Team\n",
      "string\n",
      "['कोई', 'अच्छी', 'सी']\n",
      "torch.Size([6, 96])\n",
      "bbb torch.Size([6, 96, 768])\n",
      "ppp torch.Size([6, 768])\n"
     ]
    }
   ],
   "source": [
    "path = 'google/muril-base-cased'\n",
    "## Loading the model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path,output_hidden_states=True)\n",
    "\n",
    "question = \"कोई अच्छी सी\"\n",
    "table_id = \"1-10015132-11\"\n",
    "word_to_char_start = [0, 4, 9]\n",
    "example_tokens = [\"कोई\", \"अच्छी\", \"सी\"]\n",
    "column_meta = [[\"Player\", \"string\", None], [\"No.\", \"string\", None], [\"Nationality\", \"string\", None], [\"Position\", \"string\", None], [\"Years in Toronto\", \"string\", None], [\"School/Club Team\", \"string\", \"Butler CC (KS)\"]]\n",
    "max_total_length = 96\n",
    "\n",
    "input_feature = InputFeature(\n",
    "    question,\n",
    "    table_id,\n",
    "    [],\n",
    "    word_to_char_start,\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    []\n",
    ")\n",
    "\n",
    "for column, col_type, _ in column_meta:\n",
    "    # get query tokens\n",
    "    print(column)\n",
    "    print(col_type)\n",
    "    tokens = []\n",
    "    word_to_subword = []\n",
    "    subword_to_word = []\n",
    "    for i, query_token in enumerate(example_tokens):\n",
    "        sub_tokens = tokenizer.tokenize(query_token)\n",
    "        cur_pos = len(tokens)\n",
    "        if len(sub_tokens) > 0:\n",
    "            word_to_subword += [(cur_pos, cur_pos + len(sub_tokens))]\n",
    "            tokens.extend(sub_tokens)\n",
    "            subword_to_word.extend([i] * len(sub_tokens))\n",
    "    print(tokens)\n",
    "    # tokens = [str2(token) for token in tokens]\n",
    "    # tokenize_result = tokenizer.encode_plus(\n",
    "    #     question,\n",
    "    #     return_tensors=\"pt\")\n",
    "    tokenize_result = tokenizer.encode_plus(\n",
    "        col_type + \" \" + column,\n",
    "        question,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_total_length,\n",
    "        truncation_strategy=\"longest_first\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    input_ids = tokenize_result[\"input_ids\"]\n",
    "    input_mask = tokenize_result[\"attention_mask\"]\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    column_token_length = 0\n",
    "    for i, token_id in enumerate(input_ids):\n",
    "        if token_id == tokenizer.sep_token_id:\n",
    "            column_token_length = i + 1\n",
    "            break\n",
    "    segment_ids = tokenize_result[\"token_type_ids\"]\n",
    "\n",
    "    subword_to_word = [0] * column_token_length + subword_to_word\n",
    "    word_to_subword = [(pos[0]+column_token_length, pos[1]+column_token_length) for pos in word_to_subword]\n",
    "\n",
    "    assert len(input_ids) == max_total_length\n",
    "    assert len(input_mask) == max_total_length\n",
    "    assert len(segment_ids) == max_total_length\n",
    "\n",
    "    input_feature.tokens.append(tokens)\n",
    "    input_feature.word_to_subword.append(word_to_subword)\n",
    "    input_feature.subword_to_word.append(subword_to_word)\n",
    "    input_feature.input_ids.append(input_ids)\n",
    "    input_feature.input_mask.append(input_mask)\n",
    "    input_feature.segment_ids.append(segment_ids)\n",
    "\n",
    "\n",
    "model_inputs = {k: [] for k in [\"input_ids\", \"input_mask\", \"segment_ids\"]}\n",
    "model_inputs[\"input_ids\"].extend(input_feature.input_ids)\n",
    "model_inputs[\"input_mask\"].extend(input_feature.input_mask)\n",
    "model_inputs[\"segment_ids\"].extend(input_feature.segment_ids)\n",
    "\n",
    "for k in model_inputs:\n",
    "    model_inputs[k] = np.array(model_inputs[k], dtype=np.int64)\n",
    "it = {k: torch.from_numpy(model_inputs[k][:]) for k in [\"input_ids\", \"input_mask\", \"segment_ids\"]}\n",
    "            \n",
    "print(it[\"input_ids\"].size())\n",
    "bert_output, pooled_output, _ = model(\n",
    "                input_ids=it[\"input_ids\"],\n",
    "                attention_mask=it[\"input_mask\"],\n",
    "                token_type_ids=it[\"segment_ids\"],\n",
    "                return_dict=False)\n",
    "print(\"bbb\", bert_output.shape)\n",
    "print(\"ppp\", pooled_output.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
